{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Cat and Dog Classification using CNN**"
      ],
      "metadata": {
        "id": "M7wAyH_us59U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ldlVCBCiBgX"
      },
      "source": [
        "Dataset URL: https://www.kaggle.com/datasets/salader/dogs-vs-cats/data\n",
        "\n",
        "Create API token from Kaggle Account\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5Ag1INhbjRAh"
      },
      "outputs": [],
      "source": [
        "# create directory\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc62dd7siEas",
        "outputId": "6e3e9414-1100-4fbd-e0ea-bf31e56137e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading dogs-vs-cats.zip to /content\n",
            " 99% 1.05G/1.06G [00:09<00:00, 277MB/s]\n",
            "100% 1.06G/1.06G [00:09<00:00, 116MB/s]\n"
          ]
        }
      ],
      "source": [
        "# download the dataset : Copy API command\n",
        "\n",
        "!kaggle datasets download -d salader/dogs-vs-cats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-mdOu2CvoBTt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z_x8wBFMjp0f"
      },
      "outputs": [],
      "source": [
        "# unzip the dataset folder\n",
        "\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/dogs-vs-cats.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mkjHVEt2kItR"
      },
      "outputs": [],
      "source": [
        "# import the libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
        "#from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZzAuA6fllOL",
        "outputId": "962f6b4f-a219-477f-e2de-1c3ec07e10c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# generators- divide data into batches\n",
        "\n",
        "train_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory = '/content/train',\n",
        "    labels = 'inferred',\n",
        "    label_mode = 'int',\n",
        "    batch_size = 32,\n",
        "    image_size = (256,256)\n",
        ")\n",
        "\n",
        "validation_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory = '/content/test',\n",
        "    labels = 'inferred',\n",
        "    label_mode = 'int',\n",
        "    batch_size = 32,\n",
        "    image_size = (256,256)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V-1WN7KgnD4n"
      },
      "outputs": [],
      "source": [
        "# Normalize - (0,255) --> (0,1)\n",
        "\n",
        "def process(image,label):\n",
        "  image = tf.cast(image/255, tf.float32)\n",
        "  return image, label\n",
        "\n",
        "train_ds = train_ds.map(process)\n",
        "validation_ds = validation_ds.map(process)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1"
      ],
      "metadata": {
        "id": "v41NgDGroei3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8NoLDoQVntyE"
      },
      "outputs": [],
      "source": [
        "# create CNN MODEL\n",
        "\n",
        "model1 = Sequential()\n",
        "\n",
        "model1.add(Conv2D(32, kernel_size=(3,3), padding = 'valid', activation = 'relu', input_shape = (256,256,3)))\n",
        "model1.add(MaxPooling2D(pool_size = (2,2), strides = 2, padding = 'valid'))\n",
        "\n",
        "model1.add(Conv2D(64, kernel_size=(3,3), padding = 'valid', activation = 'relu'))\n",
        "model1.add(MaxPooling2D(pool_size = (2,2), strides = 2, padding = 'valid'))\n",
        "\n",
        "model1.add(Conv2D(128, kernel_size=(3,3), padding = 'valid', activation = 'relu'))\n",
        "model1.add(MaxPooling2D(pool_size = (2,2), strides = 2, padding = 'valid'))\n",
        "\n",
        "model1.add(Flatten())\n",
        "\n",
        "model1.add(Dense(128, activation = 'relu'))\n",
        "model1.add(Dense(64, activation = 'relu'))\n",
        "model1.add(Dense(1, activation = 'sigmoid'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC-Dx_m2plqz",
        "outputId": "69066c07-ff31-40f4-8fdb-b1bb6a2bdd9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 127, 127, 32)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 62, 62, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 60, 60, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 30, 30, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 115200)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               14745728  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14847297 (56.64 MB)\n",
            "Trainable params: 14847297 (56.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Quw8uv5jpoFt"
      },
      "outputs": [],
      "source": [
        "model1.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E-sy-S4p0oS",
        "outputId": "6a9de6f9-d13b-4f8d-98f4-7b614ade0458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 65s 85ms/step - loss: 0.6545 - accuracy: 0.6161 - val_loss: 0.5830 - val_accuracy: 0.6838\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 59s 95ms/step - loss: 0.5039 - accuracy: 0.7514 - val_loss: 0.4585 - val_accuracy: 0.7860\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 52s 82ms/step - loss: 0.3993 - accuracy: 0.8167 - val_loss: 0.5223 - val_accuracy: 0.7894\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 55s 87ms/step - loss: 0.2759 - accuracy: 0.8824 - val_loss: 0.5239 - val_accuracy: 0.7934\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 54s 86ms/step - loss: 0.1528 - accuracy: 0.9393 - val_loss: 0.8190 - val_accuracy: 0.7892\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 52s 83ms/step - loss: 0.0912 - accuracy: 0.9667 - val_loss: 1.0178 - val_accuracy: 0.7768\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 51s 81ms/step - loss: 0.0639 - accuracy: 0.9779 - val_loss: 1.1038 - val_accuracy: 0.7748\n"
          ]
        }
      ],
      "source": [
        "history = model1.fit(train_ds, epochs=10, validation_data = validation_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOU-fDkAp905"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='red', label = 'train')\n",
        "plt.plot(history.history['val_accuracy'], color='blue', label = 'validation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# accuracy is increse but validation accuracy is less\n",
        "# overfitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hvk3rWZosMp4"
      },
      "outputs": [],
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(history.history['loss'], color='red', label = 'train')\n",
        "plt.plot(history.history['val_loss'], color='blue', label = 'validation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# loss is decrese but validation loss is increase\n",
        "# overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r01DnBcBvnxO"
      },
      "outputs": [],
      "source": [
        "# prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WqzJB-rvrgC"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "test_img = cv2.imread('/content/cat.jpg')\n",
        "\n",
        "plt.imshow(test_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lsrOIYnvrUB"
      },
      "outputs": [],
      "source": [
        "test_img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSzhih1RwfV6"
      },
      "outputs": [],
      "source": [
        "test_img = cv2.resize(test_img, (256,256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3ggNgHnwfSW"
      },
      "outputs": [],
      "source": [
        "test_input = test_img.reshape(1, 256, 256, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e8zB9Ygw37p"
      },
      "outputs": [],
      "source": [
        "model1.predict(test_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ways to Reduce Overfitting\n",
        "\n",
        "  1 Add more data\n",
        "\n",
        "  2 Data Auegumentation\n",
        "  \n",
        "  3 L1/ L2 regularization\n",
        "  \n",
        "  4 Dropout\n",
        "  \n",
        "  5 Batch Norm\n",
        "  \n",
        "  6 Reduce Complexity"
      ],
      "metadata": {
        "id": "wGAnCCfconnv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5Xs26hPtWtR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXGM81kqQl1U"
      },
      "source": [
        "# Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-nMV5dXtwEk"
      },
      "outputs": [],
      "source": [
        "# model 2- Adding Batch Norm and Dropout\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Conv2D(64, kernel_size=(3,3), padding = 'valid', activation = 'relu', input_shape = (256,256,3)))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(MaxPooling2D(pool_size = (2,2), strides = 2, padding = 'valid'))\n",
        "\n",
        "model2.add(Conv2D(64, kernel_size=(3,3), padding = 'valid', activation = 'relu'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(MaxPooling2D(pool_size = (2,2), strides = 2, padding = 'valid'))\n",
        "\n",
        "model2.add(Conv2D(128, kernel_size=(3,3), padding = 'valid', activation = 'relu'))\n",
        "model2.add(Conv2D(64, kernel_size=(3,3), padding = 'valid', activation = 'relu'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(MaxPooling2D(pool_size = (2,2), strides = 2, padding = 'valid'))\n",
        "\n",
        "model2.add(Conv2D(256, kernel_size=(3,3), padding = 'valid', activation = 'relu'))\n",
        "model2.add(Conv2D(64, kernel_size=(3,3), padding = 'valid', activation = 'relu'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(MaxPooling2D(pool_size = (2,2), strides = 2, padding = 'valid'))\n",
        "\n",
        "\n",
        "model2.add(Flatten())\n",
        "\n",
        "model2.add(Dense(128, activation = 'relu'))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(64, activation = 'relu'))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(32, activation = 'relu'))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(1, activation = 'sigmoid'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YQWpo9cxHrb"
      },
      "outputs": [],
      "source": [
        "model2.summary()\n",
        "model2.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSBUO3qAxKNL"
      },
      "outputs": [],
      "source": [
        "history = model2.fit(train_ds, epochs=20, validation_data = validation_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrQDUzO5urw7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='red', label = 'train')\n",
        "plt.plot(history.history['val_accuracy'], color='blue', label = 'validation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.title('Loss')\n",
        "plt.plot(history.history['loss'], color='red', label = 'train')\n",
        "plt.plot(history.history['val_loss'], color='blue', label = 'validation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6CuHn0CXjjv"
      },
      "source": [
        "Overtting Occured.(best result on training data but not on test data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlVxcUs1Qsk4"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mawgcdx0OfMO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdJuzQhPurtb"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# this is the augmentation configuration we will use for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "# this is the augmentation configuration we will use for testing:\n",
        "# only rescaling\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# this is a generator that will read pictures found in\n",
        "# subfolers of 'content/train', and indefinitely generate\n",
        "# batches of augmented image data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/train',  # target directory\n",
        "        target_size=(256, 256),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
        "\n",
        "# this is a similar generator, for validation data\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        '/content/test',\n",
        "        target_size=(256, 256),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX7X7IDNLvVI"
      },
      "outputs": [],
      "source": [
        "# model 3\n",
        "\n",
        "model3 = Sequential()\n",
        "\n",
        "model3.add(Conv2D(32, kernel_size=(3,3), padding = 'valid', activation = 'relu', input_shape = (256,256,3)))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(MaxPooling2D(pool_size = (2,2), strides = 2, padding = 'valid'))\n",
        "\n",
        "model3.add(Conv2D(64, kernel_size=(3,3), padding = 'valid', activation = 'relu'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(MaxPooling2D(pool_size = (2,2), strides = 2, padding = 'valid'))\n",
        "\n",
        "model3.add(Conv2D(128, kernel_size=(3,3), padding = 'valid', activation = 'relu'))\n",
        "model3.add(Conv2D(64, kernel_size=(3,3), padding = 'valid', activation = 'relu'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(MaxPooling2D(pool_size = (2,2), strides = 2, padding = 'valid'))\n",
        "\n",
        "model3.add(Conv2D(256, kernel_size=(3,3), padding = 'valid', activation = 'relu'))\n",
        "model3.add(Conv2D(64, kernel_size=(3,3), padding = 'valid', activation = 'relu'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(MaxPooling2D(pool_size = (2,2), strides = 2, padding = 'valid'))\n",
        "\n",
        "model3.add(Flatten())\n",
        "\n",
        "model3.add(Dense(128, activation = 'relu'))\n",
        "model3.add(Dropout(0.15))\n",
        "model3.add(Dense(64, activation = 'relu'))\n",
        "model3.add(Dropout(0.15))\n",
        "model3.add(Dense(32, activation = 'relu'))\n",
        "model3.add(Dropout(0.15))\n",
        "model3.add(Dense(1, activation = 'sigmoid'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAWXGaQJLyjl"
      },
      "outputs": [],
      "source": [
        "model3.summary()\n",
        "model3.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlKLpziSurrH"
      },
      "outputs": [],
      "source": [
        "# model 3\n",
        "\n",
        "history = model3.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=20000 // batch_size,\n",
        "        epochs=25,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=5000 // batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_WzU8slc6IH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='red', label = 'train')\n",
        "plt.plot(history.history['val_accuracy'], color='blue', label = 'validation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.title('Loss')\n",
        "plt.plot(history.history['loss'], color='red', label = 'train')\n",
        "plt.plot(history.history['val_loss'], color='blue', label = 'validation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS-d4vWvc54Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy4RcyQoYACA"
      },
      "outputs": [],
      "source": [
        "model3.save_weights('model3.h5')  # always save your weights after training or during training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "\n",
        "  * Data Augmentation adds some noise in the data which act as a Regularization factor. Maybe this is the reason Validation accuracy is better than training accuracy.\n",
        "  * So, compare to above 3 models, model 3 generalized model."
      ],
      "metadata": {
        "id": "D7N-iPVdTtjb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf1YviPRurn4"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(model3,open('model3.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "vN_8tRgBVLTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "\n",
        "Now we'll try using transfer learning, VGG16"
      ],
      "metadata": {
        "id": "or8QvxmGVkRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16"
      ],
      "metadata": {
        "id": "EXN0XV6PVZW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = VGG16(\n",
        "    weights = 'imagenet',\n",
        "    include_top= False,\n",
        "    input_shape=(256,256,3)\n",
        ")\n"
      ],
      "metadata": {
        "id": "d138ZRucVy8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model 4\n",
        "\n",
        "model4 = Sequential()\n",
        "\n",
        "model4.add(vgg)\n",
        "\n",
        "model4.add(Flatten())\n",
        "\n",
        "model4.add(Dense(128, activation='relu'))\n",
        "model4.add(Dense(64, activation='relu'))\n",
        "model4.add(Dense(32, activation='relu'))\n",
        "model4.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "zcv3rfEVWNmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.summary()"
      ],
      "metadata": {
        "id": "ZCM6W2PFW1fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.compile(optimizer='Adam', loss='binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "1LjwlUUjX77c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model 4\n",
        "\n",
        "history = model4.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=500,\n",
        "        epochs=15,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=5000 // batch_size\n",
        "        )"
      ],
      "metadata": {
        "id": "pz5dOXKVYMUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(model4,open('model4.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "ErdOHYemYZ8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGz6dI1s2WoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UWp5EEYA2Wk-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}